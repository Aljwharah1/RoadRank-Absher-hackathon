{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b9ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. IMPORTS\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c246fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\HP\\RoadRank-Absher-hackathon\n",
      "Loading datasets from: c:\\Users\\HP\\RoadRank-Absher-hackathon\\data\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. LOAD YOUR DATASETS\n",
    "# ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate to project root (parent of Model/ directory)\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"Model\" else Path.cwd()\n",
    "data_dir = project_root / \"data\"\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Loading datasets from:\", data_dir)\n",
    "\n",
    "# Example:\n",
    "df_synth = pd.read_excel(data_dir / \"Trip Summary.xlsx\")          # synthetic generated driver behavior\n",
    "df_acc = pd.read_excel(data_dir / \"Traffic Accident Statistics.xlsx\")                  # accident real data\n",
    "df_env = pd.read_excel(data_dir / \"Riyadh Roadway Environment.xlsx\")            # number of signs, lanes, lights, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c668a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_synth columns: ['trip_id', 'driver_id', 'timestamp', 'safe_driving_score', 'driver_category', 'driver_category_ar', 'avg_speed', 'max_speed', 'harsh_brakes_count', 'harsh_accels_count', 'lane_changes_count', 'speeding_percentage', 'avg_congestion', 'avg_visibility', 'road_type', 'actual_driver_type', 'time_of_day', 'weather', 'recommendation', 'recommendation_ar']\n",
      "df_synth shape: (200, 20)\n",
      "\n",
      "df_acc columns: ['السنه', 'الشهر', 'حادث تلفيات', 'حادث اصابات', 'حادث وفيات', ' مجموع عدد الحوادث', 'السبت', 'الا حد', 'الاثنين', 'الثلاثاء', 'الا ربعاء', 'الخميس', 'الجمعة', 'مجموع عدد الحوادث لجميع الأيام', 'نهارا', 'ليلا', 'مجموع عدد الحوادث حسب الوقت', 'داخل المدينة', 'خارج المدينة', 'مجموع عدد الحوادث حسب مكان الحادث', 'صغيرة', 'جيب', 'حافلة', 'ونيت', 'نقل', 'وايت', 'اخرى', 'مجموع عدد السيارات المشتركة في الحوادث حسب الطراز', -18, '18+', '30+', '40+', '50+', 'المجموع حسب العمر', 'سعودي ', 'اجنبي', 'المجموع حسب الجنسية', 'متزوج', 'اعزب', 'المجموع حسب الحالة الاجتماعية', 'متعلم ', 'امى', 'المجموع حسب الحالة التعليمية', 'نوع الرخصة خصوصى', 'نوع الرخصة عمومى', 'نوع الرخصة اليات ', 'نوع الرخصة دراجة', 'السائق لا يحمل رخصة قيادة', 'المجموع', 'السائق لا يحمل رخصة قيادة.1', 'الرخصة سارية ', 'الرخصة منتهية', 'المجموع.1', 'تصادم مع سيارة', 'تصادم مع جسم ثابت', 'دهس مشاة', 'دهس حيوان', 'حريق', 'انقلاب', 'خروج عن الطريق', 'أخرى', 'المجموع.2', 'توقف غير نظامى', 'دوران غير نظامى', 'تجاوز غير نظامى', 'عدم التقيد بالاشارة', 'السرعة الزائدة', 'السائق تحت تأثير مخدر', 'اخرى.1', 'مجموع الحوادث حسب سبب الحادث', 'عدد المصابين', 'عدد المتوفين (القتلى)', 'سليم', 'مجموع الأشخاص المشمولين بالحوادث']\n",
      "df_acc shape: (40, 74)\n",
      "\n",
      "df_env columns: ['Month ', 'Quarter', 'Year ', 'Group', 'Item', 'Unit', 'Number ']\n",
      "df_env shape: (164, 7)\n",
      "\n",
      "Warning: No common merge key found. Using df_synth only.\n",
      "Final shape: (200, 20)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. MERGE DATASETS (inspect and adapt based on actual columns)\n",
    "# ---------------------------------------------------------\n",
    "# First, let's inspect the columns in each dataframe\n",
    "print(\"df_synth columns:\", df_synth.columns.tolist())\n",
    "print(\"df_synth shape:\", df_synth.shape)\n",
    "print()\n",
    "print(\"df_acc columns:\", df_acc.columns.tolist())\n",
    "print(\"df_acc shape:\", df_acc.shape)\n",
    "print()\n",
    "print(\"df_env columns:\", df_env.columns.tolist())\n",
    "print(\"df_env shape:\", df_env.shape)\n",
    "print()\n",
    "\n",
    "# For now, merge on the common column 'driver_id' if available\n",
    "# Adjust the merge keys based on the actual columns available\n",
    "if 'driver_id' in df_synth.columns and 'driver_id' in df_env.columns:\n",
    "    df = df_synth.merge(df_env, on=\"driver_id\", how=\"left\", suffixes=('_synth', '_env'))\n",
    "else:\n",
    "    # If no common key, just use the first dataframe\n",
    "    print(\"Warning: No common merge key found. Using df_synth only.\")\n",
    "    df = df_synth.copy()\n",
    "\n",
    "if 'driver_id' in df.columns and 'driver_id' in df_acc.columns:\n",
    "    df = df.merge(df_acc, on=\"driver_id\", how=\"left\", suffixes=('_main', '_acc'))\n",
    "\n",
    "print(\"Final shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89fcbdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns:\n",
      "['driver_id', 'safe_driving_score', 'driver_category', 'driver_category_ar', 'avg_speed', 'max_speed', 'harsh_brakes_count', 'harsh_accels_count', 'lane_changes_count', 'speeding_percentage', 'avg_congestion', 'avg_visibility', 'road_type', 'actual_driver_type', 'time_of_day', 'weather', 'recommendation', 'recommendation_ar']\n",
      "\n",
      "Numeric columns: ['safe_driving_score', 'avg_speed', 'max_speed', 'harsh_brakes_count', 'harsh_accels_count', 'lane_changes_count', 'speeding_percentage', 'avg_congestion', 'avg_visibility']\n",
      "Features shape: (200, 17)\n",
      "Target shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. SELECT FEATURES + TARGET\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Print all columns to identify the target\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "print()\n",
    "\n",
    "# Use a numeric column as target (e.g., safe_driving_score if available, otherwise the first numeric column)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Choose target column (adjust based on actual column names)\n",
    "if 'safe_driving_score' in df.columns:\n",
    "    TARGET = 'safe_driving_score'\n",
    "elif len(numeric_cols) > 0:\n",
    "    TARGET = numeric_cols[0]\n",
    "    print(f\"Using '{TARGET}' as target (first numeric column)\")\n",
    "else:\n",
    "    print(\"No numeric columns found!\")\n",
    "    TARGET = None\n",
    "\n",
    "if TARGET is not None:\n",
    "    # Drop rows where target is missing\n",
    "    df = df.dropna(subset=[TARGET])\n",
    "    \n",
    "    # Example of removing irrelevant columns\n",
    "    drop_cols = [\"trip_id\", \"timestamp\", \"trip_summary\", \"index\"]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "    \n",
    "    # Split features / labels\n",
    "    X = df.drop(columns=[TARGET])\n",
    "    y = df[TARGET]\n",
    "    \n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "else:\n",
    "    print(\"Cannot proceed without a target column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72ae799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns encoded: []\n",
      "Final X dtypes:\n",
      "driver_id              float64\n",
      "driver_category        float64\n",
      "driver_category_ar     float64\n",
      "avg_speed              float64\n",
      "max_speed              float64\n",
      "harsh_brakes_count     float64\n",
      "harsh_accels_count     float64\n",
      "lane_changes_count     float64\n",
      "speeding_percentage    float64\n",
      "avg_congestion         float64\n",
      "avg_visibility         float64\n",
      "road_type              float64\n",
      "actual_driver_type     float64\n",
      "time_of_day            float64\n",
      "weather                float64\n",
      "recommendation         float64\n",
      "recommendation_ar      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. HANDLE CATEGORICAL FEATURES\n",
    "# ---------------------------------------------------------\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Ensure all columns are numeric (int or float)\n",
    "X = X.astype({col: 'float64' for col in X.columns if X[col].dtype == 'int64'})\n",
    "\n",
    "print(f\"Categorical columns encoded: {list(categorical_cols)}\")\n",
    "print(f\"Final X dtypes:\\n{X.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "731e65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6. TRAIN / TEST SPLIT\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b5c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7. TRAIN XGBOOST MODEL\n",
    "# ---------------------------------------------------------\n",
    "model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "677319a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "MODEL PERFORMANCE\n",
      "RMSE: 5.275619891446808\n",
      "MAE: 2.712159679790144\n",
      "R² Score: 0.9743347355773767\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 8. EVALUATE MODEL\n",
    "# ---------------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R² Score:\", r2)\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9e30b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & encoders saved successfully.\n",
      "- Model saved to: xgboost_model.joblib\n",
      "- Encoders saved to: encoders.joblib\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 9. SAVE MODEL + ENCODERS\n",
    "# ---------------------------------------------------------\n",
    "# Save the model directly using joblib for better compatibility\n",
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model_path = \"xgboost_model.joblib\"\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save encoders\n",
    "encoders_path = \"encoders.joblib\"\n",
    "joblib.dump(encoders, encoders_path)\n",
    "\n",
    "print(\"Model & encoders saved successfully.\")\n",
    "print(f\"- Model saved to: {model_path}\")\n",
    "print(f\"- Encoders saved to: {encoders_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
